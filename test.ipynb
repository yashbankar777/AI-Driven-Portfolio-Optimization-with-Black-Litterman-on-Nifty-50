{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data for 46 stocks.\n",
      "Selected 26 top candidate stocks for optimization\n",
      "\n",
      "=== ENHANCED PORTFOLIO OPTIMIZATION RESULTS ===\n",
      "\n",
      "Portfolio Expected Return: 0.1500 (15.00%)\n",
      "Portfolio Volatility: 0.1512 (15.12%)\n",
      "Sharpe Ratio: 0.5292\n",
      "\n",
      "Top 15 Recommended Stocks:\n",
      "                      Ticker  Weight  Expected Return  Volatility  Sharpe Ratio\n",
      "SUNPHARMA.NS    SUNPHARMA.NS   15.00            18.27       18.95          1.92\n",
      "ITC.NS                ITC.NS   12.87            12.00       18.71          0.17\n",
      "DIVISLAB.NS      DIVISLAB.NS   12.54            23.15       24.97          1.83\n",
      "TCS.NS                TCS.NS    7.44            12.00       20.83          0.13\n",
      "PIDILITIND.NS  PIDILITIND.NS    7.02            12.00       21.74          0.24\n",
      "INFY.NS              INFY.NS    3.44            12.89       22.68          0.69\n",
      "ICICIBANK.NS    ICICIBANK.NS    3.13            12.94       20.64          0.87\n",
      "BHARTIARTL.NS  BHARTIARTL.NS    2.57            18.66       23.26          1.83\n",
      "WIPRO.NS            WIPRO.NS    2.00            16.78       28.37          0.93\n",
      "CIPLA.NS            CIPLA.NS    2.00            12.00       25.26          0.60\n",
      "AMBUJACEM.NS    AMBUJACEM.NS    2.00            12.00       35.60          0.06\n",
      "M&M.NS                M&M.NS    2.00            22.31       31.32          1.72\n",
      "ADANIPORTS.NS  ADANIPORTS.NS    2.00            12.00       41.80          0.31\n",
      "SBIN.NS              SBIN.NS    2.00            12.07       29.48          0.57\n",
      "JSWSTEEL.NS      JSWSTEEL.NS    2.00            12.00       26.85          0.02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.optimize as sco\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Nifty 50 Stocks List\n",
    "nifty50_tickers = [\n",
    "    \"RELIANCE.NS\", \"TCS.NS\", \"HDFCBANK.NS\", \"INFY.NS\", \"ICICIBANK.NS\", \n",
    "    \"HINDUNILVR.NS\", \"SBIN.NS\", \"ITC.NS\", \"BHARTIARTL.NS\", \"KOTAKBANK.NS\",\n",
    "    \"AXISBANK.NS\", \"BAJFINANCE.NS\", \"SUNPHARMA.NS\", \"BAJAJFINSV.NS\", \"ADANIPORTS.NS\",\n",
    "    \"MARUTI.NS\", \"M&M.NS\", \"POWERGRID.NS\", \"NTPC.NS\", \"DRREDDY.NS\",\n",
    "    \"CIPLA.NS\", \"COALINDIA.NS\", \"JSWSTEEL.NS\", \"TITAN.NS\", \n",
    "    \"ULTRACEMCO.NS\", \"LT.NS\", \"TECHM.NS\", \"NESTLEIND.NS\", \"GRASIM.NS\",\n",
    "    \"ASIANPAINT.NS\", \"SHREECEM.NS\", \"UPL.NS\", \"BAJAJ-AUTO.NS\", \"AMBUJACEM.NS\",\n",
    "    \"INDUSINDBK.NS\", \"BRITANNIA.NS\", \"DABUR.NS\", \"WIPRO.NS\",\n",
    "    \"ZOMATO.NS\", \"PIDILITIND.NS\", \"DIVISLAB.NS\", \"CHOLAFIN.NS\", \"ADANIENT.NS\",\n",
    "    \"TATACONSUM.NS\", \"ADANIGREEN.NS\", \"SAIL.NS\", \"GODREJCP.NS\"\n",
    "]\n",
    "\n",
    "class EnhancedPortfolioOptimizer:\n",
    "    def __init__(self, tickers, start_date=\"2020-01-01\", end_date=\"2025-01-01\", \n",
    "                 risk_free_rate=0.07, momentum_window=126, sentiment_factor=0.08):\n",
    "        \"\"\"\n",
    "        Enhanced Portfolio Optimizer with advanced analytics\n",
    "        \n",
    "        Parameters:\n",
    "        - tickers: List of stock tickers\n",
    "        - start_date/end_date: Analysis period\n",
    "        - risk_free_rate: Risk-free rate (higher for emerging markets like India)\n",
    "        - momentum_window: Days to consider for momentum calculation\n",
    "        - sentiment_factor: Factor to adjust returns based on market sentiment\n",
    "        \"\"\"\n",
    "        self.tickers = tickers\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "        self.momentum_window = momentum_window\n",
    "        self.sentiment_factor = sentiment_factor\n",
    "        \n",
    "        # Get market index for reference\n",
    "        self.market_index = yf.download(\"^NSEI\", start=start_date, end=end_date, progress=False)\n",
    "        \n",
    "        # Fetch and process stock data\n",
    "        self.fetch_stock_data()\n",
    "        self.prepare_analytics()\n",
    "    \n",
    "    def fetch_stock_data(self):\n",
    "        \"\"\"Get historical data for all valid tickers\"\"\"\n",
    "        valid_tickers = []\n",
    "        valid_data = pd.DataFrame()\n",
    "        \n",
    "        for ticker in self.tickers:\n",
    "            try:\n",
    "                stock_data = yf.download(\n",
    "                    ticker, \n",
    "                    start=self.start_date, \n",
    "                    end=self.end_date, \n",
    "                    progress=False\n",
    "                )\n",
    "                \n",
    "                if not stock_data.empty and len(stock_data) > 100:\n",
    "                    valid_tickers.append(ticker)\n",
    "                    valid_data[ticker] = stock_data['Close']\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {ticker}: {e}\")\n",
    "        \n",
    "        # Update with valid data\n",
    "        self.tickers = valid_tickers\n",
    "        self.price_data = valid_data.dropna(axis=1, thresh=len(valid_data)*0.7)\n",
    "        self.price_data = self.price_data.ffill()  # Forward fill missing values\n",
    "        \n",
    "        # Verify data\n",
    "        if self.price_data.empty or len(self.price_data.columns) < 5:\n",
    "            raise ValueError(\"Insufficient valid stock data available.\")\n",
    "        \n",
    "        print(f\"Successfully loaded data for {len(self.price_data.columns)} stocks.\")\n",
    "    \n",
    "    def prepare_analytics(self):\n",
    "        \"\"\"Prepare comprehensive analytics for portfolio optimization\"\"\"\n",
    "        # Calculate returns\n",
    "        full_returns = np.log(self.price_data / self.price_data.shift(1)).dropna()\n",
    "        \n",
    "        # Use shorter window (1 year) for more relevant analysis\n",
    "        self.returns = full_returns.iloc[-252:]  # Last year of trading days\n",
    "        \n",
    "        # Calculate volatility and other statistics\n",
    "        self.daily_volatility = self.returns.std()\n",
    "        self.annual_volatility = self.daily_volatility * np.sqrt(252)\n",
    "        self.annual_returns = self.returns.mean() * 252\n",
    "        \n",
    "        # Calculate correlation and covariance\n",
    "        self.correlation = self.returns.corr()\n",
    "        self.covariance = self.returns.cov() * 252  # Annualized\n",
    "        \n",
    "        # Calculate advanced metrics\n",
    "        self.calculate_advanced_metrics()\n",
    "    \n",
    "    def calculate_advanced_metrics(self):\n",
    "        \"\"\"Calculate advanced metrics for better portfolio construction\"\"\"\n",
    "        # 1. Calculate momentum scores\n",
    "        self.momentum_scores = self.calculate_momentum()\n",
    "        \n",
    "        # 2. Calculate volatility clustering\n",
    "        self.volatility_clusters = self.cluster_by_volatility()\n",
    "        \n",
    "        # 3. Calculate drawdown resilience\n",
    "        self.drawdown_metrics = self.calculate_max_drawdowns()\n",
    "        \n",
    "        # 4. Regime-based performance\n",
    "        self.regime_performance = self.calculate_regime_performance()\n",
    "        \n",
    "        # 5. Calculate factor exposures\n",
    "        self.factor_exposures = self.calculate_factor_exposures()\n",
    "        \n",
    "        # 6. Calculate Sharpe ratios\n",
    "        self.sharpe_ratios = self.calculate_sharpe_ratios()\n",
    "        \n",
    "        # 7. Create composite score\n",
    "        self.composite_scores = self.calculate_composite_scores()\n",
    "        \n",
    "        # 8. Calculate enhanced returns\n",
    "        self.enhanced_returns = self.calculate_enhanced_returns()\n",
    "        \n",
    "        # 9. Select top candidates\n",
    "        self.select_top_candidates()\n",
    "    \n",
    "    def calculate_momentum(self):\n",
    "        \"\"\"Calculate price momentum over specified window\"\"\"\n",
    "        # Use more recent shorter-term momentum for stronger signal\n",
    "        \n",
    "        # Check if enough data is available for momentum calculation\n",
    "        if len(self.price_data) < 63:\n",
    "            # Not enough data for short term momentum, use what's available\n",
    "            short_momentum = (self.price_data.iloc[-1] / self.price_data.iloc[0] - 1)\n",
    "        else:\n",
    "            short_momentum = (self.price_data.iloc[-1] / self.price_data.iloc[-63] - 1)  # ~3 months\n",
    "        \n",
    "        # Check if enough data for long-term momentum\n",
    "        if len(self.price_data) < self.momentum_window:\n",
    "            # Not enough data, use the same as short momentum\n",
    "            long_momentum = short_momentum\n",
    "        else:\n",
    "            long_momentum = (self.price_data.iloc[-1] / self.price_data.iloc[-self.momentum_window] - 1)  # ~6 months\n",
    "        \n",
    "        # Combine short and long momentum (emphasize short-term)\n",
    "        return 0.6 * short_momentum + 0.4 * long_momentum\n",
    "    \n",
    "    def cluster_by_volatility(self):\n",
    "        \"\"\"Cluster stocks by volatility patterns using a safer approach\"\"\"\n",
    "        try:\n",
    "            # Prepare volatility time series \n",
    "            vol_series = self.returns.rolling(window=21).std() * np.sqrt(252)\n",
    "            \n",
    "            # Create a simpler approach that avoids DataFrame indexing issues\n",
    "            # Just use the last available volatility value for each stock as a feature\n",
    "            if not vol_series.empty:\n",
    "                # Get the last valid volatility for each stock\n",
    "                last_volatility = vol_series.iloc[-1].fillna(vol_series.mean())\n",
    "                \n",
    "                # Simple clustering based on volatility level\n",
    "                if len(last_volatility) >= 3:\n",
    "                    # Use quantiles to define clusters\n",
    "                    low_vol = last_volatility.quantile(0.33)\n",
    "                    high_vol = last_volatility.quantile(0.67)\n",
    "                    \n",
    "                    # Assign clusters based on volatility levels\n",
    "                    clusters = pd.Series(index=last_volatility.index)\n",
    "                    clusters[last_volatility <= low_vol] = 0  # Low volatility cluster\n",
    "                    clusters[(last_volatility > low_vol) & (last_volatility <= high_vol)] = 1  # Medium volatility\n",
    "                    clusters[last_volatility > high_vol] = 2  # High volatility\n",
    "                    \n",
    "                    return clusters\n",
    "            \n",
    "            # Fallback for insufficient data\n",
    "            return pd.Series(0, index=self.returns.columns)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in volatility clustering: {e}\")\n",
    "            # Return default clusters\n",
    "            return pd.Series(0, index=self.returns.columns)\n",
    "    \n",
    "    def calculate_max_drawdowns(self):\n",
    "        \"\"\"Calculate maximum drawdowns to assess resilience\"\"\"\n",
    "        drawdowns = {}\n",
    "        \n",
    "        for ticker in self.price_data.columns:\n",
    "            # Calculate running maximum\n",
    "            running_max = self.price_data[ticker].cummax()\n",
    "            # Calculate drawdown\n",
    "            drawdown = (self.price_data[ticker] / running_max - 1)\n",
    "            # Get max drawdown\n",
    "            drawdowns[ticker] = drawdown.min()\n",
    "            \n",
    "        return pd.Series(drawdowns)\n",
    "    \n",
    "    def calculate_regime_performance(self):\n",
    "        \"\"\"Analyze performance across different market regimes\"\"\"\n",
    "        # Calculate simplified regime performance to avoid indexing issues\n",
    "        regime_performance = {}\n",
    "        \n",
    "        try:\n",
    "            # Calculate market returns\n",
    "            market_returns = np.log(self.market_index['Close'] / self.market_index['Close'].shift(1)).dropna()\n",
    "            \n",
    "            # Use a simpler approach to define bull and bear markets\n",
    "            if len(market_returns) >= 21:  # At least 21 days of data\n",
    "                # Use 21-day moving average to identify regimes\n",
    "                rolling_avg = market_returns.rolling(window=21).mean()\n",
    "                bull_dates = rolling_avg[rolling_avg > 0].index\n",
    "                bear_dates = rolling_avg[rolling_avg < 0].index\n",
    "                \n",
    "                # Analyze each ticker\n",
    "                for ticker in self.returns.columns:\n",
    "                    ticker_returns = self.returns[ticker]\n",
    "                    \n",
    "                    # Performance in bull market (shared dates between ticker_returns and bull_dates)\n",
    "                    common_bull_dates = ticker_returns.index.intersection(bull_dates)\n",
    "                    if len(common_bull_dates) > 0:\n",
    "                        bull_performance = ticker_returns.loc[common_bull_dates].mean() * 252\n",
    "                    else:\n",
    "                        bull_performance = 0\n",
    "                    \n",
    "                    # Performance in bear market\n",
    "                    common_bear_dates = ticker_returns.index.intersection(bear_dates)\n",
    "                    if len(common_bear_dates) > 0:\n",
    "                        bear_performance = ticker_returns.loc[common_bear_dates].mean() * 252\n",
    "                    else:\n",
    "                        bear_performance = 0\n",
    "                    \n",
    "                    # Reward stocks that perform well in bear markets\n",
    "                    regime_score = bull_performance + (2.5 * bear_performance)\n",
    "                    regime_performance[ticker] = regime_score\n",
    "            else:\n",
    "                # Not enough data for market regimes, use annual returns as proxy\n",
    "                for ticker in self.returns.columns:\n",
    "                    regime_performance[ticker] = self.annual_returns[ticker]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in regime performance calculation: {e}\")\n",
    "            # Fallback: use annual returns\n",
    "            for ticker in self.returns.columns:\n",
    "                regime_performance[ticker] = self.annual_returns[ticker]\n",
    "        \n",
    "        return pd.Series(regime_performance)\n",
    "    \n",
    "    def calculate_factor_exposures(self):\n",
    "        \"\"\"Use PCA to identify factor exposures\"\"\"\n",
    "        # Safety check for empty data\n",
    "        if self.returns.empty:\n",
    "            return pd.Series(1, index=self.returns.columns)\n",
    "            \n",
    "        # Apply PCA to returns\n",
    "        try:\n",
    "            # Fill missing values to avoid PCA errors\n",
    "            filled_returns = self.returns.fillna(0)\n",
    "            \n",
    "            # Determine number of components\n",
    "            n_components = min(5, len(filled_returns.columns), len(filled_returns) - 1)\n",
    "            \n",
    "            # Apply PCA\n",
    "            pca = PCA(n_components=max(1, n_components))\n",
    "            pca.fit(filled_returns)\n",
    "            \n",
    "            # Get stock exposures to principal components\n",
    "            exposures = pd.DataFrame(\n",
    "                pca.components_.T, \n",
    "                index=filled_returns.columns,\n",
    "                columns=[f'Factor_{i+1}' for i in range(pca.n_components_)]\n",
    "            )\n",
    "            \n",
    "            # Calculate diversification score (lower exposure to common factors is better)\n",
    "            div_score = exposures.abs().sum(axis=1)\n",
    "            return 1 / div_score  # Invert so higher is better\n",
    "        except Exception as e:\n",
    "            # Return default values if PCA fails\n",
    "            print(f\"PCA calculation failed: {e}\")\n",
    "            return pd.Series(1, index=self.returns.columns)\n",
    "    \n",
    "    def calculate_sharpe_ratios(self):\n",
    "        \"\"\"Calculate Sharpe ratios to filter low-performing stocks\"\"\"\n",
    "        excess_returns = self.annual_returns - self.risk_free_rate\n",
    "        sharpe_ratios = excess_returns / self.annual_volatility\n",
    "        return sharpe_ratios\n",
    "    \n",
    "    def calculate_composite_scores(self):\n",
    "        \"\"\"Create composite score combining momentum, regime performance, and drawdown\"\"\"\n",
    "        # Normalize metrics to 0-1 range\n",
    "        norm_momentum = normalize_series(self.momentum_scores)\n",
    "        norm_regime = normalize_series(self.regime_performance)\n",
    "        \n",
    "        # For drawdowns, lower is better, so invert\n",
    "        inverted_drawdown = -self.drawdown_metrics\n",
    "        norm_drawdown = normalize_series(inverted_drawdown)\n",
    "        \n",
    "        # Combine into weighted composite score\n",
    "        composite_scores = (\n",
    "            0.45 * norm_momentum +     # Emphasize momentum\n",
    "            0.35 * norm_regime +       # Regime performance\n",
    "            0.20 * norm_drawdown       # Drawdown resilience\n",
    "        )\n",
    "        \n",
    "        return composite_scores\n",
    "    \n",
    "    def calculate_enhanced_returns(self):\n",
    "        \"\"\"Calculate forward-looking enhanced returns\"\"\"\n",
    "        # Start with base annualized returns (but with less weight)\n",
    "        base_returns = self.annual_returns.copy()\n",
    "        \n",
    "        # Calculate earnings growth proxy (using price momentum as substitute)\n",
    "        growth_proxy = self.momentum_scores\n",
    "        \n",
    "        # Normalize all factors to 0-1 range\n",
    "        norm_momentum = normalize_series(self.momentum_scores)\n",
    "        norm_drawdown = normalize_series(-self.drawdown_metrics)\n",
    "        norm_regime = normalize_series(self.regime_performance)\n",
    "        norm_factor = normalize_series(self.factor_exposures)\n",
    "                      \n",
    "        # Forward-looking return estimates - more heavily weighted toward predictive factors\n",
    "        enhanced_returns = (\n",
    "            0.25 * base_returns +            # Historical returns (less weight)\n",
    "            0.30 * (growth_proxy * 0.2) +    # Growth proxy (scaled)\n",
    "            0.25 * (norm_momentum * 0.25) +  # Momentum contribution\n",
    "            0.10 * (norm_drawdown * 0.15) +  # Drawdown resilience\n",
    "            0.10 * (norm_regime * 0.20)      # Market regime performance\n",
    "        )\n",
    "        \n",
    "        # Ensure minimum return threshold (12%) for candidate stocks\n",
    "        return enhanced_returns.clip(lower=0.12)\n",
    "    \n",
    "    def select_top_candidates(self):\n",
    "        \"\"\"Select top candidates for optimization based on composite score\"\"\"\n",
    "        # Filter stocks with negative Sharpe ratios\n",
    "        viable_stocks = self.sharpe_ratios[self.sharpe_ratios > 0].index.tolist()\n",
    "        \n",
    "        # Select top N stocks based on composite score\n",
    "        n_top = min(40, len(viable_stocks))  # Select top 40 or less if not enough viable stocks\n",
    "        \n",
    "        # Get scores only for viable stocks\n",
    "        viable_scores = self.composite_scores[viable_stocks]\n",
    "        \n",
    "        # Select top candidates\n",
    "        self.top_candidates = viable_scores.sort_values(ascending=False).head(n_top).index.tolist()\n",
    "        \n",
    "        print(f\"Selected {len(self.top_candidates)} top candidate stocks for optimization\")\n",
    "        \n",
    "        # Filter relevant data to top candidates only\n",
    "        self.filtered_returns = self.enhanced_returns[self.top_candidates]\n",
    "        self.filtered_covariance = self.covariance.loc[self.top_candidates, self.top_candidates]\n",
    "\n",
    "    def optimize_for_target_return(self, target_return=0.15):\n",
    "        \"\"\"Optimize portfolio to achieve target return with minimum risk\"\"\"\n",
    "        # Use top candidates only\n",
    "        n_assets = len(self.top_candidates)\n",
    "        \n",
    "        def portfolio_volatility(weights):\n",
    "            return np.sqrt(np.dot(weights.T, np.dot(self.filtered_covariance, weights)))\n",
    "        \n",
    "        def portfolio_return(weights):\n",
    "            return np.sum(self.filtered_returns * weights)\n",
    "        \n",
    "        def target_function(weights):\n",
    "            # Minimize volatility\n",
    "            return portfolio_volatility(weights)\n",
    "        \n",
    "        # Constraints\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},  # Weights sum to 1\n",
    "            {'type': 'ineq', 'fun': lambda x: portfolio_return(x) - target_return}  # Min return >= target\n",
    "        ]\n",
    "        \n",
    "        # Bounds - more flexible weights (2% to 15% per stock)\n",
    "        bounds = tuple((0.02, 0.15) for _ in range(n_assets))\n",
    "        \n",
    "        # Initial guess - give higher weight to highest momentum stocks\n",
    "        momentum_ranks = self.momentum_scores[self.top_candidates].rank()\n",
    "        initial_weights = momentum_ranks / momentum_ranks.sum()\n",
    "        \n",
    "        try:\n",
    "            # Try to achieve target return\n",
    "            result = sco.minimize(\n",
    "                target_function, \n",
    "                initial_weights, \n",
    "                method='SLSQP', \n",
    "                bounds=bounds, \n",
    "                constraints=constraints\n",
    "            )\n",
    "            \n",
    "            if result['success']:\n",
    "                return result['x']\n",
    "            else:\n",
    "                # If can't achieve target, maximize return\n",
    "                return self.optimize_for_max_return()\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization for target return failed: {e}\")\n",
    "            # If optimization fails, use max return approach\n",
    "            return self.optimize_for_max_return()\n",
    "    \n",
    "    def optimize_for_max_return(self, max_volatility=0.25):\n",
    "        \"\"\"Optimize portfolio for max return within volatility constraint\"\"\"\n",
    "        # Use top candidates only\n",
    "        n_assets = len(self.top_candidates)\n",
    "        \n",
    "        def portfolio_volatility(weights):\n",
    "            return np.sqrt(np.dot(weights.T, np.dot(self.filtered_covariance, weights)))\n",
    "        \n",
    "        def negative_return(weights):\n",
    "            # Maximize return (minimize negative return)\n",
    "            return -np.sum(self.filtered_returns * weights)\n",
    "        \n",
    "        # Constraints\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},  # Weights sum to 1\n",
    "            {'type': 'ineq', 'fun': lambda x: max_volatility - portfolio_volatility(x)}  # Max volatility\n",
    "        ]\n",
    "        \n",
    "        # Bounds (2% to 15% per stock)\n",
    "        bounds = tuple((0.02, 0.15) for _ in range(n_assets))\n",
    "        \n",
    "        # Initial guess - use momentum ranking\n",
    "        momentum_ranks = self.momentum_scores[self.top_candidates].rank()\n",
    "        initial_weights = momentum_ranks / momentum_ranks.sum()\n",
    "        \n",
    "        try:\n",
    "            result = sco.minimize(\n",
    "                negative_return, \n",
    "                initial_weights, \n",
    "                method='SLSQP', \n",
    "                bounds=bounds, \n",
    "                constraints=constraints\n",
    "            )\n",
    "            \n",
    "            if result['success']:\n",
    "                return result['x']\n",
    "            else:\n",
    "                # If optimization fails, use equal weights\n",
    "                print(\"Optimization for max return failed, using equal weights\")\n",
    "                return np.ones(n_assets) / n_assets\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization for max return failed: {e}\")\n",
    "            # If optimization fails, use equal weights\n",
    "            return np.ones(n_assets) / n_assets\n",
    "    \n",
    "    def generate_optimized_portfolio(self, target_return=0.15):\n",
    "        \"\"\"Generate optimized portfolio with comprehensive metrics\"\"\"\n",
    "        # Optimize weights for target return (using only top candidates)\n",
    "        optimal_weights = self.optimize_for_target_return(target_return)\n",
    "        \n",
    "        # Create results dataframe\n",
    "        portfolio = pd.DataFrame({\n",
    "            'Ticker': self.top_candidates,\n",
    "            'Weight': optimal_weights,\n",
    "            'Expected Return': self.filtered_returns,\n",
    "            'Volatility': self.annual_volatility[self.top_candidates],\n",
    "            'Momentum Score': self.momentum_scores[self.top_candidates],\n",
    "            'Drawdown Resilience': self.drawdown_metrics[self.top_candidates],\n",
    "            'Regime Performance': self.regime_performance[self.top_candidates],\n",
    "            'Sharpe Ratio': self.sharpe_ratios[self.top_candidates]\n",
    "        })\n",
    "        \n",
    "        # Sort by weight\n",
    "        portfolio = portfolio.sort_values('Weight', ascending=False)\n",
    "        \n",
    "        # Calculate portfolio metrics\n",
    "        portfolio_return = np.sum(portfolio['Weight'] * portfolio['Expected Return'])\n",
    "        portfolio_volatility = np.sqrt(\n",
    "            np.dot(portfolio['Weight'], np.dot(\n",
    "                self.filtered_covariance, \n",
    "                portfolio['Weight']\n",
    "            ))\n",
    "        )\n",
    "        sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_volatility\n",
    "        \n",
    "        print(\"\\n=== ENHANCED PORTFOLIO OPTIMIZATION RESULTS ===\")\n",
    "        print(f\"\\nPortfolio Expected Return: {portfolio_return:.4f} ({portfolio_return*100:.2f}%)\")\n",
    "        print(f\"Portfolio Volatility: {portfolio_volatility:.4f} ({portfolio_volatility*100:.2f}%)\")\n",
    "        print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
    "        \n",
    "        # Show top stocks\n",
    "        top_n = min(15, len(portfolio))\n",
    "        print(f\"\\nTop {top_n} Recommended Stocks:\")\n",
    "        display_portfolio = portfolio.head(top_n).copy()\n",
    "        display_portfolio['Weight'] = display_portfolio['Weight'] * 100  # Convert to percentage\n",
    "        display_portfolio['Expected Return'] = display_portfolio['Expected Return'] * 100  # Convert to percentage\n",
    "        display_portfolio['Volatility'] = display_portfolio['Volatility'] * 100  # Convert to percentage\n",
    "        print(display_portfolio[['Ticker', 'Weight', 'Expected Return', 'Volatility', 'Sharpe Ratio']].to_string(float_format=lambda x: f\"{x:.2f}\"))\n",
    "        \n",
    "        return portfolio\n",
    "\n",
    "# Helper function to normalize a series to 0-1 range\n",
    "def normalize_series(series):\n",
    "    \"\"\"Normalize a series to 0-1 range\"\"\"\n",
    "    if series.max() == series.min():\n",
    "        return pd.Series(0.5, index=series.index)\n",
    "    return (series - series.min()) / (series.max() - series.min() + 1e-10)\n",
    "\n",
    "def main():\n",
    "    # Create enhanced optimizer with India-appropriate parameters\n",
    "    optimizer = EnhancedPortfolioOptimizer(\n",
    "        tickers=nifty50_tickers,  # Using only the Nifty 50 universe\n",
    "        start_date=\"2020-01-01\",\n",
    "        end_date=\"2025-01-01\",\n",
    "        risk_free_rate=0.07,  # Higher risk-free rate for India\n",
    "        sentiment_factor=0.08  # Increased sentiment impact\n",
    "    )\n",
    "    \n",
    "    # Generate optimized portfolio with target of 15% return\n",
    "    portfolio = optimizer.generate_optimized_portfolio(target_return=0.15)\n",
    "    \n",
    "    return portfolio\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
